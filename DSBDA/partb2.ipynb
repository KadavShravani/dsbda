{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e13124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54dac4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning 1 \n",
    "punctuations = '''!()-{}[];:\"'\\,<>./?@#$%^&*_~`'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9029723",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"Hello!!!, he said ...and went.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b03ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67563dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello he said and wentH\n",
      "Hello he said and wentHe\n",
      "Hello he said and wentHel\n",
      "Hello he said and wentHell\n",
      "Hello he said and wentHello\n",
      "Hello he said and wentHello \n",
      "Hello he said and wentHello h\n",
      "Hello he said and wentHello he\n",
      "Hello he said and wentHello he \n",
      "Hello he said and wentHello he s\n",
      "Hello he said and wentHello he sa\n",
      "Hello he said and wentHello he sai\n",
      "Hello he said and wentHello he said\n",
      "Hello he said and wentHello he said \n",
      "Hello he said and wentHello he said a\n",
      "Hello he said and wentHello he said an\n",
      "Hello he said and wentHello he said and\n",
      "Hello he said and wentHello he said and \n",
      "Hello he said and wentHello he said and w\n",
      "Hello he said and wentHello he said and we\n",
      "Hello he said and wentHello he said and wen\n",
      "Hello he said and wentHello he said and went\n"
     ]
    }
   ],
   "source": [
    "for char in my_str:\n",
    " if(char not in punctuations):\n",
    "    no_punct = no_punct + char\n",
    "    print(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef442746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello he said and went\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning 2\n",
    "import re\n",
    "s = \"Hello!!!, he said ...and went.\"\n",
    "s = re.sub(r'[^\\w\\s]','',s)\n",
    "# not of word and space character\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "028c8d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Pravin, how are you doing today?', 'The weather in lonavala is rainy.', 'The sky is full of cloud.']\n",
      "['Hello', 'Mr.', 'Pravin', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'in', 'lonavala', 'is', 'rainy', '.', 'The', 'sky', 'is', 'full', 'of', 'cloud', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aj538\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokanization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "example_text = \"Hello Mr. Pravin, how are you doing today? The weather in lonavala is rainy. The sky is full of cloud.\"\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f82ceda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aj538\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1e214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0fe6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'several', 'types', 'of', 'stemming', 'algorithms', '.']\n",
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aj538\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#steming\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "input_str = \"There are several types of stemming algorithms.\"\n",
    "input_str = nltk.word_tokenize(input_str)\n",
    "print (input_str)\n",
    "for word in input_str:\n",
    " print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb1d1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aj538\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'several', 'cities', 'with', 'mice', '.']\n",
      "There\n",
      "are\n",
      "several\n",
      "city\n",
      "with\n",
      "mouse\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Lamitization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "input_str = \"There are several cities with mice.\"\n",
    "input_str = nltk.word_tokenize(input_str)\n",
    "print (input_str)\n",
    "for word in input_str:\n",
    " print(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c102e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
